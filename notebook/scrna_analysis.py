# -*- coding: utf-8 -*-
"""scRNA_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1McHrAdp08WkJ-Qjo13wbqm8m0vUgxeWe

# Single-Cell RNA-seq Analysis with Scanpy
Google Colab Notebook
"""

# Colab: install required packages. This cell may take a few minutes.
!pip install --quiet scanpy anndata numpy pandas matplotlib seaborn scikit-misc leidenalg igraph statsmodels decoupler

# Optional (for pathway analysis): omnipath / gseapy available via pip if needed
!pip install --quiet omnipath

from google.colab import drive
drive.mount('/content/drive')

# Where I'll save outputs in my Drive
DRIVE_SAVE_DIR = "/content/drive/MyDrive/scRNAseq_Scanpy_Results"
import os
os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)
print("Saving outputs to:", DRIVE_SAVE_DIR)

"""### Loading `bone_marrow.h5ad` from Google Drive

To load the `bone_marrow.h5ad` file, you need to provide the correct path to it within your Google Drive. Follow these steps:

1.  **Locate your file in Google Drive**: Open your Google Drive, navigate to where `bone_marrow.h5ad` is stored, right-click on the file, and select "Get link". Copy the path from the link or note the exact folder structure.
2.  **Update `DATA_PATH`**: In the code cell `GBwh3E-FaUfZ` below, replace `"/content/drive/MyDrive/path/to/bone_marrow.h5ad"` with the actual path to your file (e.g., `"/content/drive/MyDrive/Your_Folder/bone_marrow.h5ad"`). Make sure to keep the quotes around the path.
3.  **Run the cell**:

"""

# Option: If the file is already in Drive, specify path:
DATA_PATH = "/content/drive/MyDrive/scRNAseq_Scanpy_Results/bone_marrow.h5ad"
# The provided URL below is a drive link, if you wish to use it, you would typically download the file from this link
# and then specify its path in the DATA_PATH variable.
# https://drive.google.com/file/d/1bYR7dOcRaTXh1rue316WYq1clHALwB5v/view?usp=drive_link

import scanpy as sc
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Plot settings
sc.settings.figdir = DRIVE_SAVE_DIR
sc.set_figure_params(dpi=120, frameon=False, figsize=(6,5))

# Utility: safe save
def save_fig(fig, fname):
    path = Path(DRIVE_SAVE_DIR) / fname
    fig.savefig(path, bbox_inches='tight')
    print("Saved:", path)

def write_csv(df, fname):
    path = Path(DRIVE_SAVE_DIR) / fname
    df.to_csv(path, index=True)
    print("Saved:", path)

"""## Load Data"""

print("Reading AnnData from:", DATA_PATH)
adata = sc.read_h5ad(DATA_PATH)
adata

def run_qc(adata, min_genes=200, min_cells=3, mito_prefixes=['MT-','mt-'], max_mito_pct=20.0):
    ad = adata.copy()
    # Compute standard QC metrics
    sc.pp.calculate_qc_metrics(ad, inplace=True)
    # Identify mitochondrial genes robustly
    mt_mask = ad.var_names.str.upper().str.startswith('MT-')
    if mt_mask.sum() > 0:
        ad.obs['pct_counts_mt'] = (ad[:, mt_mask].X.sum(axis=1).A1 / ad.obs['total_counts']) * 100
    else:
        ad.obs['pct_counts_mt'] = 0.0
    # Basic filters
    sc.pp.filter_cells(ad, min_genes=min_genes)
    sc.pp.filter_genes(ad, min_cells=min_cells)
    # mito filter
    ad = ad[ad.obs['pct_counts_mt'] < max_mito_pct].copy()
    return ad

# Run QC
adata = run_qc(adata, min_genes=200, min_cells=3, max_mito_pct=25.0)
adata

def preprocess(adata, n_top_genes=3000, target_sum=1e4):
    ad = adata.copy()
    sc.pp.normalize_total(ad, target_sum=target_sum)
    sc.pp.log1p(ad)
    sc.pp.highly_variable_genes(ad, flavor='seurat_v3', n_top_genes=n_top_genes)
    ad = ad[:, ad.var['highly_variable']].copy()
    sc.pp.scale(ad, max_value=10)
    return ad

adata = preprocess(adata, n_top_genes=3000)

def compute_embedding_and_clusters(adata, n_pcs=40, n_neighbors=10, umap_min_dist=0.5, leiden_resolution=1.0):
    ad = adata.copy()
    sc.tl.pca(ad, svd_solver='arpack', n_comps=n_pcs)
    sc.pp.neighbors(ad, n_neighbors=n_neighbors, n_pcs=n_pcs)
    sc.tl.umap(ad, min_dist=umap_min_dist)
    sc.tl.leiden(ad, resolution=leiden_resolution, key_added='leiden')
    return ad

adata = compute_embedding_and_clusters(adata, n_pcs=40, n_neighbors=10, umap_min_dist=0.5, leiden_resolution=1.0)

from pathlib import Path
import matplotlib.pyplot as plt

# UMAP blank
sc.pl.umap(adata, color=None, show=False)
plt.gcf().savefig(Path(DRIVE_SAVE_DIR)/"umap_blank.png", bbox_inches='tight')

# UMAP clusters (leiden)
sc.pl.umap(adata, color='leiden', save="_clusters.png", show=False)
# The above Scanpy save will write to sc.settings.figdir; also write copy to DRIVE_SAVE_DIR
print("UMAP files saved to:", DRIVE_SAVE_DIR)

# If you have final celltype labels later, we will save that UMAP too.

# Find markers (Wilcoxon)
sc.tl.rank_genes_groups(adata, groupby='leiden', method='wilcoxon', key_added='rank_genes_wilcox')
# Save top markers table
def save_top_markers(adata, n_top=10, fname="cluster_markers_top10.csv"):
    groups = adata.uns['rank_genes_wilcox']['names'].dtype.names
    rows = []
    for g in groups:
        names = adata.uns['rank_genes_wilcox']['names'][g][:n_top]
        scores = adata.uns['rank_genes_wilcox']['scores'][g][:n_top]
        for name, score in zip(names, scores):
            rows.append({'cluster': g, 'gene': name, 'score': float(score)})
    df = pd.DataFrame(rows)
    write_csv(df, fname)
    return df

markers_df = save_top_markers(adata, n_top=10)
markers_df.head()

"""## Download Ensembl Gene ID Mapping

### Subtask:
Download the Ensembl human gene ID to external gene name mapping table using `wget` and save it as 'result.txt'.

**Reasoning**:
The subtask is to download a file using `wget`. I will use a code block to execute the `wget` command with the specified URL and output filename.

## Load and Format Ensembl Mapping (Retry 2: Use PyBiomart)

### Subtask:
Obtain human gene ID to external gene name mapping from Ensembl BioMart using `pybiomart`, process it into a pandas DataFrame, and ensure it has 'ensembl_gene_id' and 'gene_name' columns.

**Reasoning**:
The first step to use `pybiomart` is to install the library. I will use `!pip install pybiomart` to install it.
"""

!pip install pybiomart

"""**Reasoning**:
Now that `pybiomart` is installed, I will import it, connect to the Ensembl BioMart service, select the human gene dataset, query for Ensembl gene ID and external gene name, convert the results to a pandas DataFrame, rename the columns, and display the first few rows to fulfill the subtask requirements.


"""

import pybiomart
import pandas as pd

# Connect to the Ensembl BioMart service
# It's good practice to try different mirrors if the default fails
# biomart = pybiomart.Mart(host='www.ensembl.org', port=80, path='/biomart/martservice') # Default
# biomart = pybiomart.Mart(host='uswest.ensembl.org', port=80, path='/biomart/martservice') # US West mirror
try:
    # Using the default server first
    dataset = pybiomart.Dataset(name='hsapiens_gene_ensembl', host='www.ensembl.org')
    print("Successfully connected to Ensembl BioMart dataset.")
except Exception as e:
    print(f"Could not connect to default Ensembl host: {e}. Trying an alternative.")
    try:
        # Try a common alternative mirror
        dataset = pybiomart.Dataset(name='hsapiens_gene_ensembl', host='uswest.ensembl.org')
        print("Successfully connected to alternative Ensembl BioMart dataset.")
    except Exception as e_alt:
        print(f"Could not connect to alternative Ensembl host: {e_alt}. Please check internet connection or BioMart service availability.")
        dataset = None # Set dataset to None if both fail

if dataset:
    # Define the attributes to retrieve: 'ensembl_gene_id' and 'external_gene_name'
    # Ensure the attribute names match those expected by pybiomart
    attributes = ['ensembl_gene_id', 'external_gene_name']

    # Execute the query to fetch the data from BioMart
    print(f"Fetching data for attributes: {attributes}...")
    try:
        df_gene_mapping = dataset.query(attributes=attributes)
        print("Data fetched successfully.")

        # Rename columns to 'ensembl_gene_id' and 'gene_name'
        df_gene_mapping.columns = ['ensembl_gene_id', 'gene_name']

        # Display the first few rows for verification
        print("\nFirst 5 rows of the gene mapping DataFrame:")
        display(df_gene_mapping.head())
    except Exception as e:
        print(f"Error querying or processing data from BioMart: {e}")
        df_gene_mapping = pd.DataFrame() # Create an empty DataFrame on error
else:
    print("Failed to connect to any Ensembl BioMart server. Skipping data retrieval.")
    df_gene_mapping = pd.DataFrame() # Ensure df_gene_mapping is defined even if connection fails

"""## Query and Preprocess PanglaoDB Markers

### Subtask:
Import `decoupler`, query Omnipath for PanglaoDB markers for 'human', remove duplicated entries, and rename columns to 'source' and 'target' to fit `decoupler`'s expected format. Display the first few rows.

**Reasoning**:
The first instruction is to install `decoupler` if it's not already installed. This is a common first step when using a new library in a Colab environment.
"""

pip install decoupler

"""**Reasoning**:
Now that `decoupler` is installed, I will import it, query Omnipath for PanglaoDB markers for 'human', remove duplicated entries, and rename the columns to 'source' and 'target' as required by the subtask. Finally, I will display the first few rows for verification.


"""

!wget wget -O result.txt 'http://www.ensembl.org/biomart/martservice?query=<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE Query><Query  virtualSchemaName = "default" formatter = "CSV" header = "0" uniqueRows = "0" count = "" datasetConfigVersion = "0.6" ><Dataset name = "hsapiens_gene_ensembl" interface = "default" ><Attribute name = "ensembl_gene_id" /><Attribute name = "external_gene_name" /></Dataset></Query>'


#this downloads the table of genes directly from ensemble

import pandas as pd

ensembl_var = pd.read_csv('/content/result.txt', header = None)

ensembl_var.columns = ['ensembl_gene_id', 'gene_name']

ensembl_var.head(3)

"""# Task
Add `import decoupler as dc` to the current cell, and then use `dc.op.resource` to query Omnipath for human PanglaoDB markers. Afterwards, remove duplicated entries, rename the columns to 'source' and 'target', and display the first few rows of the resulting DataFrame. If `dc.op.resource` fails after the import correction, modify the code to use `decoupler.read_panglaodb` for fetching PanglaoDB markers.

## Correct the import statement for `decoupler`

### Subtask:
Add `import decoupler as dc` to the cell to define the `dc` alias, resolving the `NameError`.

**Reasoning**:
The subtask requires adding `import decoupler as dc` to resolve the `NameError` and updating subsequent calls to use the `dc` alias. I will regenerate the code cell with this modification.
"""

import decoupler as dc # Import decoupler with alias dc

# Query Omnipath and get PanglaoDB
markers = dc.op.resource(name="PanglaoDB", organism="human")

# Keep canonical cell type markers alone
#markers = markers[markers["canonical_marker"]]

# Remove duplicated entries
markers = markers[~markers.duplicated(["cell_type", "genesymbol"])]

#Format because dc only accepts cell_type and genesymbol

markers = markers.rename(columns={"cell_type": "source", "genesymbol": "target"})
markers = markers[["source", "target"]]


markers.head()

"""## Summary:

### Q&A
The `dc.op.resource` function did not fail after the import correction, therefore, it was not necessary to modify the code to use `decoupler.read_panglaodb`.

### Data Analysis Key Findings
*   The `decoupler` library was successfully imported with the alias `dc`.
*   Human PanglaoDB markers were retrieved effectively using `dc.op.resource(name="PanglaoDB", organism="human")`.
*   Duplicated entries within the retrieved markers were successfully identified and removed based on "cell\_type" and "genesymbol" columns.
*   The DataFrame columns were correctly renamed to 'source' and 'target' as specified, preparing the data for further analysis.

### Insights or Next Steps
*   The `markers` DataFrame is now cleaned and formatted, making it ready for downstream analysis or integration into a larger data processing pipeline.
*   The successful direct use of `dc.op.resource` demonstrates its utility for accessing curated biological resources like PanglaoDB.

"""

#correct target to ensemble
markers = markers.merge(ensembl_var, left_on="target", right_on="gene_name", how="left")
markers = markers.drop(columns=["target"])
# Remove duplicated entries
markers = markers[~markers.duplicated(["source", "ensembl_gene_id"])]

#Format because dc only accepts cell_type and genesymbol
markers = markers.rename(columns={"source": "source", "ensembl_gene_id": "target"})

markers = markers[["source", "target"]]
markers = markers.dropna()

markers.head()

"""### Correcting Canonical Markers to Ensembl IDs

The `annotate_by_markers` function expects the gene names in `canonical_markers` to match the gene identifiers present in `adata.var_names`. Since `adata.var_names` likely contains Ensembl IDs, we need to convert the gene symbols in `canonical_markers` to Ensembl IDs using the `ensembl_var` mapping.

This process involves:
1.  Creating a reverse mapping from gene names to Ensembl IDs from `ensembl_var`.
2.  Iterating through `canonical_markers` and replacing gene symbols with their Ensembl IDs.
3.  Filtering out any markers that couldn't be mapped (i.e., not found in `ensembl_var`).
"""

import numpy as np

# Create a gene name to Ensembl ID mapping dictionary
gene_name_to_ensembl = pd.Series(ensembl_var.ensembl_gene_id.values, index=ensembl_var.gene_name).to_dict()

# Prepare new canonical markers with Ensembl IDs
canonical_markers_ensembl = {}
for cell_type, gene_symbols in canonical_markers.items():
    ensembl_ids = []
    for symbol in gene_symbols:
        ensembl_id = gene_name_to_ensembl.get(symbol)
        if ensembl_id and ensembl_id in adata.var_names: # Check if Ensembl ID exists and is in adata
            ensembl_ids.append(ensembl_id)
    if ensembl_ids:
        canonical_markers_ensembl[cell_type] = ensembl_ids

# Print the new canonical markers to verify
print("Canonical markers mapped to Ensembl IDs:")
for ct, genes in canonical_markers_ensembl.items():
    print(f"{ct}: {genes}")

# Re-run annotation with Ensembl-mapped markers
ann_df = annotate_by_markers(adata, canonical_markers_ensembl)
ann_df.to_csv(Path(DRIVE_SAVE_DIR)/"cluster_annotation_suggestions_ensembl.csv", index=False)

print("\nUpdated annotation suggestions:")
display(ann_df.head())

"""### Defining `annotate_by_markers` function

The `annotate_by_markers` function is crucial for scoring and assigning cell types to clusters based on provided marker genes. Since it was not defined, we provide a standard implementation that calculates average expression scores for marker genes within each cluster and assigns the cell type with the highest score.
"""

def annotate_by_markers(adata_obj, canonical_markers_dict, min_score_threshold=0.1):
    """
    Annotates clusters in an AnnData object based on canonical marker genes.

    Args:
        adata_obj: The AnnData object with clusters (e.g., 'leiden') and normalized expression data.
        canonical_markers_dict: A dictionary where keys are cell type labels (str)
                                and values are lists of Ensembl gene IDs (str) for markers.
        min_score_threshold: Minimum score required for a cell type to be considered.

    Returns:
        A pandas DataFrame with 'cluster' and 'suggested_label' columns.
    """
    cluster_labels = adata_obj.obs['leiden'].unique().categories
    annotation_suggestions = []

    for cluster_id in cluster_labels:
        cluster_adata = adata_obj[adata_obj.obs['leiden'] == cluster_id]
        scores = {}

        for cell_type, marker_genes in canonical_markers_dict.items():
            # Filter markers that are actually present in the adata object
            # and are in the var_names (processed data)
            present_markers = [gene for gene in marker_genes if gene in adata_obj.var_names]

            if not present_markers:
                scores[cell_type] = 0
                continue

            # Get expression for present markers in the cluster from the main .X matrix
            # (assuming it's already normalized and log-transformed)
            avg_expression = cluster_adata[:, present_markers].X.mean(axis=0).flatten()

            # Simple scoring: average expression of markers
            if len(avg_expression) > 0:
                scores[cell_type] = np.mean(avg_expression)
            else:
                scores[cell_type] = 0

        # Find the cell type with the highest score
        if scores:
            # Ensure scores dictionary is not empty before finding max
            if any(score > 0 for score in scores.values()):
                max_score_type = max(scores, key=scores.get)
                max_score = scores[max_score_type]

                if max_score > min_score_threshold:
                    annotation_suggestions.append({'cluster': str(cluster_id), 'suggested_label': max_score_type})
                else:
                    annotation_suggestions.append({'cluster': str(cluster_id), 'suggested_label': 'Undetermined'})
            else:
                annotation_suggestions.append({'cluster': str(cluster_id), 'suggested_label': 'Undetermined'})
        else:
            annotation_suggestions.append({'cluster': str(cluster_id), 'suggested_label': 'Undetermined'})

    return pd.DataFrame(annotation_suggestions)

# Map suggestions to adata.obs
label_map = ann_df.set_index('cluster')['suggested_label'].to_dict()
adata.obs['celltype_suggested'] = adata.obs['leiden'].map(label_map).astype('category')

# Plot annotated UMAP and save
sc.pl.umap(adata, color='celltype_suggested', legend_loc='on data', save="_celltype_suggested.png", show=False)
print("Annotated UMAP saved to:", DRIVE_SAVE_DIR)

props = adata.obs['celltype_suggested'].value_counts(normalize=True)
counts = adata.obs['celltype_suggested'].value_counts()
pd.DataFrame({'count': counts, 'proportion': props}).to_csv(Path(DRIVE_SAVE_DIR)/"cluster_proportions.csv")
print("Saved cluster_proportions.csv")
print(pd.concat([counts, props], axis=1).rename(columns={0:'count',1:'proportion'}))

# Expected reference ranges (approximate, from literature / marrow expectations)
expected_ranges = {
    'Neutrophils': (0.50, 0.70),
    'Monocytes': (0.03, 0.07),
    'NK cells': (0.02, 0.10),
    'T cells': (0.03, 0.10),
    'B cells': (0.05, 0.20),
    'Plasma cells': (0.00, 0.03),
    'Erythroid': (0.20, 0.40),
    'Megakaryocytes': (0.01, 0.03),
    'HSC/Progenitor': (0.01, 0.03)
}

def flag_unusual(proportions, expected_ranges, fold_threshold=2.0):
    # Unusual = either > fold_threshold * expected_upper OR < expected_lower / fold_threshold
    flags = {}
    for ct, prop in proportions.items():
        if ct in expected_ranges:
            low, high = expected_ranges[ct]
            if prop > (high * fold_threshold):
                flags[ct] = f"High ({prop:.2%} > {fold_threshold}x upper={high:.0%})"
            elif prop < (low / fold_threshold):
                flags[ct] = f"Low ({prop:.2%} < 1/{fold_threshold}x lower={low:.0%})"
            else:
                flags[ct] = f"Within expected ({prop:.2%})"
        else:
            flags[ct] = f"No expectation defined ({prop:.2%})"
    return flags

prop_dict = adata.obs['celltype_suggested'].value_counts(normalize=True).to_dict()
flags = flag_unusual(prop_dict, expected_ranges, fold_threshold=2.0)
flags

from sklearn.metrics import silhouette_score, adjusted_rand_score

# Silhouette on PCA (require numeric labels)
labels = adata.obs['leiden'].astype(int)
sil = silhouette_score(adata.obsm['X_pca'][:,:20], labels)
print("Silhouette score (PCA, 20 PCs):", sil)

# Stability across resolutions (compute ARI between clusterings)
resolutions = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2]
clusterings = {}
for r in resolutions:
    sc.tl.leiden(adata, resolution=r, key_added=f"leiden_{r}")
    clusterings[r] = adata.obs[f"leiden_{r}"].astype(int).values

# Compute pairwise ARI matrix
ari_results = {}
for i, r1 in enumerate(resolutions):
    for r2 in resolutions[i+1:]:
        ari = adjusted_rand_score(clusterings[r1], clusterings[r2])
        ari_results[(r1,r2)] = ari
ari_results

# Bootstrapping approach: resample cells many times, compute ARI with original clustering for overlapping cells.
def bootstrap_cluster_stability(adata, n_bootstraps=20, sample_frac=0.8, leiden_resolution=1.0, random_seed=0):
    np.random.seed(random_seed)
    base_labels = adata.obs['leiden'].astype(str)
    ari_list = []
    for i in range(n_bootstraps):
        idx = np.random.choice(adata.n_obs, size=int(adata.n_obs * sample_frac), replace=False)
        ad_boot = adata[idx,:].copy()
        sc.pp.pca(ad_boot, svd_solver='arpack', n_comps=30)
        sc.pp.neighbors(ad_boot, n_neighbors=10, n_pcs=30)
        sc.tl.leiden(ad_boot, resolution=leiden_resolution)
        # compare labels on overlapping cells (by index)
        # need to compute ARI between clusterings for common cells -> because bootstrapped set is subset
        common_idx = ad_boot.obs_names
        ari = adjusted_rand_score(base_labels.loc[common_idx], ad_boot.obs['leiden'])
        ari_list.append(ari)
        print(f"Bootstrap {i+1}/{n_bootstraps} ARI: {ari:.3f}")
    return ari_list

# Warning: can be slow. Use n_bootstraps=20 for speed; increase if you want better estimate.
bootstrap_aris = bootstrap_cluster_stability(adata, n_bootstraps=12, sample_frac=0.8)
print("Bootstrap ARI mean:", np.mean(bootstrap_aris))

# Already ran rank_genes_groups (wilcoxon) earlier. Save DE for a cluster of interest (e.g., a cluster labeled 'NK cells')
# Example: get top genes for cluster '3'
cluster_of_interest = list(ann_df['cluster'])[0]  # just as example; replace with the cluster ID you want
sc.tl.rank_genes_groups(adata, groupby='celltype_suggested', method='wilcoxon', key_added='rank_genes_celltype')
# Convert to DataFrame for a specific group
def get_ranked_df(adata, group, key='rank_genes_celltype', n_top=50):
    names = adata.uns[key]['names'][group][:n_top]
    pvals = adata.uns[key]['pvals_adj'][group][:n_top]
    logfold = adata.uns[key]['logfoldchanges'][group][:n_top]
    df = pd.DataFrame({'gene': names, 'p_adj': pvals, 'logfold': logfold})
    return df

# Save for each celltype
groups = list(adata.uns['rank_genes_celltype']['names'].dtype.names)
for g in groups:
    df = get_ranked_df(adata, g, key='rank_genes_celltype', n_top=200)
    write_csv(df, f"DE_{g.replace('/','_')}.csv")

# A simple ISG signature check (common ISGs)
ISG_genes = ['IFIT1','IFIT2','IFIT3','MX1','ISG15','OAS1','OAS2','RSAD2','IFI6','IFI27','IFI44L']

# Create a gene name to Ensembl ID mapping dictionary (if not already available, assume ensembl_var is global)
gene_name_to_ensembl = pd.Series(ensembl_var.ensembl_gene_id.values, index=ensembl_var.gene_name).to_dict()

# Convert ISG gene symbols to Ensembl IDs
isg_ensembl_ids = []
for gene_symbol in ISG_genes:
    ensembl_id = gene_name_to_ensembl.get(gene_symbol)
    if ensembl_id: # Only add if a mapping was found
        isg_ensembl_ids.append(ensembl_id)

# Filter for ISG genes present in adata.var_names (now using Ensembl IDs)
isg_present = [g for g in isg_ensembl_ids if g in adata.var_names]
print("ISG genes present (Ensembl IDs):", isg_present)

# Compute module score for ISG signature per cell and compare across annotated cell types
# Only run if there are valid genes to score
if len(isg_present) > 0:
    sc.tl.score_genes(adata, gene_list=isg_present, score_name='ISG_score')
    # Save average ISG score per cell type
    isg_by_celltype = adata.obs.groupby('celltype_suggested')['ISG_score'].mean()
    write_csv(isg_by_celltype.rename("ISG_score").to_frame(), "ISG_by_celltype.csv")
    display(isg_by_celltype)
else:
    print("No ISG genes found in adata.var_names after Ensembl ID mapping. Skipping scoring.")

# Quick example using gseapy (GSEA) on ranked DE genes for a given cell type:
!pip install gseapy
import gseapy as gp

# Choose a group to analyze
group = groups[0]  # replace with your group of interest
rank_df = get_ranked_df(adata, group, key='rank_genes_celltype', n_top=500)

# gseapy preranked expects two columns: gene and score (use logfold)
rnk = rank_df[['gene','logfold']].dropna()

# Convert Ensembl IDs to Gene Symbols for gseapy
# Create a mapping from Ensembl ID to gene name (symbol)
ensembl_to_gene_name = pd.Series(ensembl_var.gene_name.values, index=ensembl_var.ensembl_gene_id).to_dict()

# Replace Ensembl IDs in rnk with gene symbols
rnk['gene'] = rnk['gene'].map(ensembl_to_gene_name)

# Remove any genes that couldn't be mapped to a symbol or are NaN after mapping
rnk = rnk.dropna(subset=['gene'])

# Save the preranked file
rnk.to_csv(Path(DRIVE_SAVE_DIR)/f"{group}_pre_rank.rnk", sep="\t", index=False, header=False)

# Run prerank against some gene sets (e.g., KEGG or Reactome) - this requires internet access in Colab.
prerank_res = gp.prerank(rnk= str(Path(DRIVE_SAVE_DIR)/f"{group}_pre_rank.rnk"),
                         gene_sets='KEGG_2016', outdir=DRIVE_SAVE_DIR, permutation_num=100)

print("GSEA Prerank Results for the selected group:")
display(prerank_res.res2d.head())

# Save full object with embeddings and annotations
OUT_ANN = Path(DRIVE_SAVE_DIR)/"adata_annotated.h5ad"
adata.write_h5ad(OUT_ANN)
print("Saved annotated AnnData to:", OUT_ANN)

import json

# Create a brief JSON summary
summary = {
    "n_cells": int(adata.n_obs),
    "n_genes": int(adata.n_vars),
    "n_clusters": int(adata.obs['celltype_suggested'].nunique()),
    "silhouette_pca20": float(sil),
    "bootstrap_ari_mean": float(np.mean(bootstrap_aris))
}
with open(Path(DRIVE_SAVE_DIR)/"run_summary.json","w") as fh:
    json.dump(summary, fh, indent=2)
print("Summary saved:", summary)

# Optional: zip results for downloading
!zip -r /content/scRNAseq_results.zip "{DRIVE_SAVE_DIR}" > /dev/null
print("Zipped results to /content/scRNAseq_results.zip")
